{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (1.35.7)\n",
      "Requirement already satisfied: PyPDF2 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: fpdf in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (8.1.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from openai) (2.7.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from ipywidgets) (8.26.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.11 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from ipywidgets) (4.0.11)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from ipywidgets) (3.0.11)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.18.4)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install openai PyPDF2 fpdf ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: ffmpy is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -q gradio openai pypdf2 tiktoken langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_community\n",
      "  Using cached langchain_community-0.2.6-py3-none-any.whl (2.2 MB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from langchain_community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from langchain_community) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from langchain_community) (3.9.5)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.6 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from langchain_community) (0.2.6)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.10 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from langchain_community) (0.2.10)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from langchain_community) (0.1.82)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from langchain_community) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from langchain_community) (8.4.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Using cached marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from langchain<0.3.0,>=0.2.6->langchain_community) (0.2.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from langchain<0.3.0,>=0.2.6->langchain_community) (2.7.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.10->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.10->langchain_community) (24.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (2024.6.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.6->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.6->langchain_community) (2.18.4)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain_community\n",
      "Successfully installed dataclasses-json-0.6.7 langchain_community-0.2.6 marshmallow-3.21.3 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (3.0.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: transformers in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (4.42.3)\n",
      "Collecting torch\n",
      "  Using cached torch-2.3.1-cp311-cp311-win_amd64.whl (159.8 MB)\n",
      "Requirement already satisfied: filelock in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Collecting sympy\n",
      "  Using cached sympy-1.12.1-py3-none-any.whl (5.7 MB)\n",
      "Collecting networkx\n",
      "  Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Collecting mkl<=2021.4.0,>=2021.1.1\n",
      "  Using cached mkl-2021.4.0-py2.py3-none-win_amd64.whl (228.5 MB)\n",
      "Collecting intel-openmp==2021.*\n",
      "  Using cached intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl (3.5 MB)\n",
      "Collecting tbb==2021.*\n",
      "  Using cached tbb-2021.13.0-py3-none-win_amd64.whl (286 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shubham\\onedrive\\desktop\\lawyantraopenapi\\.venv\\lib\\site-packages (from requests->transformers) (2024.6.2)\n",
      "Collecting mpmath<1.4.0,>=1.1.0\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: tbb, mpmath, intel-openmp, sympy, networkx, mkl, torch\n",
      "Successfully installed intel-openmp-2021.4.0 mkl-2021.4.0 mpmath-1.3.0 networkx-3.3 sympy-1.12.1 tbb-2021.13.0 torch-2.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install PyPDF2 transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
      "     -------------------------------------- 290.4/290.4 kB 6.1 MB/s eta 0:00:00\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-4.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pypdf PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_file_path):\n",
    "    doc = fitz.open(pdf_file_path)\n",
    "    text = \"\"\n",
    "    \n",
    "    for page in doc:\n",
    "        page_text = page.get_text()\n",
    "        text += page_text\n",
    "        \n",
    "        # If no text was extracted, use OCR on the page's image\n",
    "        if not page_text.strip():\n",
    "            pix = page.get_pixmap()\n",
    "            img = Image.open(io.BytesIO(pix.tobytes()))\n",
    "            ocr_text = pytesseract.image_to_string(img)\n",
    "            text += ocr_text\n",
    "    \n",
    "    print(f\"Extracted text length from {pdf_file_path}: {len(text)}\")\n",
    "    return text\n",
    "\n",
    "def save_text_from_pdfs(pdf_folder_path):\n",
    "    pdf_files = [f for f in os.listdir(pdf_folder_path) if f.lower().endswith('.pdf')]\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_file_path = os.path.join(pdf_folder_path, pdf_file)\n",
    "        text = extract_text_from_pdf(pdf_file_path)\n",
    "        if text:\n",
    "            output_file_path = os.path.join(TEXT_OUTPUT_FOLDER, os.path.basename(pdf_file_path).replace(\".pdf\", \".txt\"))\n",
    "            with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(text)\n",
    "            print(f\"Saved text to {output_file_path}\")\n",
    "\n",
    "save_text_from_pdfs(PDF_FOLDER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "from langchain.chains.mapreduce import MapReduceDocumentsChain\n",
    "from langchain.chains import ReduceDocumentsChain, LLMChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import Ollama\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Initialize the language model\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "# Paths and constants\n",
    "PDF_FILE_PATH = \"SCSL-03-01-T-1283 PART 1.pdf\"\n",
    "FINAL_SUMMARY_FILE = \"final_summary.txt\"\n",
    "CHUNK_SIZE = 4000\n",
    "CHUNK_OVERLAP = 200\n",
    "\n",
    "# Function to extract and split content from PDF\n",
    "def extract_and_split_content(pdf_file_path):\n",
    "    loader = PyPDFLoader(pdf_file_path)\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "# Prompts for the map and reduce stages\n",
    "map_template = \"\"\"chunks prompt here\n",
    "\n",
    "{content}\n",
    "\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "map_chain = LLMChain(prompt=map_prompt, llm=llm)\n",
    "\n",
    "reduce_template = \"\"\"The following is a set of summaries of legal documents:\n",
    "\n",
    "{doc_summaries}\n",
    "\n",
    "final promt here\n",
    "FINAL SUMMARY:\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "reduce_chain = LLMChain(prompt=reduce_prompt, llm=llm)\n",
    "\n",
    "# Setup the map-reduce chain\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
    ")\n",
    "\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    ")\n",
    "\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    llm_chain=map_chain,\n",
    "    document_variable_name=\"content\",\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    return_intermediate_steps=False,\n",
    ")\n",
    "\n",
    "# Function to process a single chunk\n",
    "def process_chunk(chunk):\n",
    "    return map_chain.run(content=chunk.page_content)\n",
    "\n",
    "# Function to summarize the PDF using map-reduce with parallel processing\n",
    "def summarize_pdf_map_reduce(pdf_file_path):\n",
    "    # Extract and split the content\n",
    "    documents = extract_and_split_content(pdf_file_path)\n",
    "   \n",
    "    # Use multiprocessing for the map step\n",
    "    with Pool() as pool:\n",
    "        chunk_summaries = pool.map(process_chunk, documents)\n",
    "   \n",
    "    # Combine the summaries\n",
    "    combined_summary = [Document(page_content=summary) for summary in chunk_summaries]\n",
    "   \n",
    "    # Final reduce step\n",
    "    final_summary = reduce_documents_chain.run(combined_summary)\n",
    "   \n",
    "    # Write the final summary to a file\n",
    "    with open(FINAL_SUMMARY_FILE, 'w', encoding='utf-8') as file:\n",
    "        file.write(final_summary)\n",
    "   \n",
    "    return final_summary\n",
    "\n",
    "# Run the summarization\n",
    "final_summary = summarize_pdf_map_reduce(PDF_FILE_PATH)\n",
    "print(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from langchain.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "# Initialize language model\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "# Paths and constants\n",
    "PDF_FILE_PATH = \"pdf\"\n",
    "FINAL_SUMMARY_FILE = \"final_summary.txt\"\n",
    "CHUNK_SIZE = 4000\n",
    "CHUNK_OVERLAP = 200\n",
    "MAX_WORKERS = 4  \n",
    "\n",
    "def extract_text_from_pdf(pdf_file_path):\n",
    "    doc = fitz.open(pdf_file_path)\n",
    "    text = \" \".join(page.get_text() for page in doc)\n",
    "    print(f\"Extracted text length: {len(text)}\")\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    print(f\"Number of chunks created: {len(chunks)}\")\n",
    "    return chunks\n",
    "\n",
    "def summarize_chunk(chunk):\n",
    "    summarize_template = \"\"\"Summarize the following text concisely,\n",
    "    {content}\n",
    "\n",
    "    SUMMARY:\"\"\"\n",
    "    summarize_prompt = PromptTemplate.from_template(summarize_template)\n",
    "    summarize_chain = LLMChain(prompt=summarize_prompt, llm=llm)\n",
    "    return summarize_chain.run(content=chunk)\n",
    "\n",
    "def summarize_chunks_parallel(chunks):\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        future_to_chunk = {executor.submit(summarize_chunk, chunk): chunk for chunk in chunks}\n",
    "        summaries = []\n",
    "        for future in as_completed(future_to_chunk):\n",
    "            summaries.append(future.result())\n",
    "    return summaries\n",
    "\n",
    "def final_summarization(combined_summary):\n",
    "    reduce_template = \"\"\"\n",
    "\n",
    "    {content}\n",
    "\n",
    "    FINAL SUMMARY:\"\"\"\n",
    "    reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "    reduce_chain = LLMChain(prompt=reduce_prompt, llm=llm)\n",
    "    return reduce_chain.run(content=combined_summary)\n",
    "\n",
    "def summarize_pdf(pdf_file_path):\n",
    "    start_time = time.time()\n",
    "\n",
    "    text = extract_text_from_pdf(pdf_file_path)\n",
    "    if not text:\n",
    "        print(\"No text extracted from PDF\")\n",
    "        return\n",
    "\n",
    "    text_chunks = chunk_text(text)\n",
    "    if not text_chunks:\n",
    "        print(\"No chunks formed\")\n",
    "        return\n",
    "\n",
    "    print(\"Summarizing chunks...\")\n",
    "    chunk_summaries = summarize_chunks_parallel(text_chunks)\n",
    "    \n",
    "    print(\"Generating final summary...\")\n",
    "    combined_summary = \" \".join(chunk_summaries)\n",
    "    final_summary = final_summarization(combined_summary)\n",
    "    \n",
    "    with open(FINAL_SUMMARY_FILE, 'w', encoding='utf-8') as file:\n",
    "        file.write(final_summary)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Total processing time: {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    return final_summary\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        final_summary = summarize_pdf(PDF_FILE_PATH)\n",
    "        if final_summary:\n",
    "            print(\"Final Summary:\")\n",
    "            print(final_summary)\n",
    "        else:\n",
    "            print(\"Summarization process failed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from langchain.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import time\n",
    "\n",
    "# Initialize the language model\n",
    "llm = Ollama(model=\"llama3\")  # Assuming Ollama is used here\n",
    "\n",
    "# Paths and constants\n",
    "TEXT_FOLDER_PATH = \"RUF/text_files\"  # Folder containing text files\n",
    "SUMMARY_OUTPUT_FOLDER = os.path.join(TEXT_FOLDER_PATH, \"../summaries\")\n",
    "os.makedirs(SUMMARY_OUTPUT_FOLDER, exist_ok=True)\n",
    "CHUNK_SIZE = 4000\n",
    "CHUNK_OVERLAP = 200\n",
    "MAX_WORKERS = 8  # Number of parallel workers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    print(f\"Number of chunks created: {len(chunks)}\")\n",
    "    return chunks\n",
    "\n",
    "def summarize_chunk(chunk):\n",
    "    summarize_template = \"\"\"\n",
    "    {content}\n",
    "\n",
    "    SUMMARY:\"\"\"\n",
    "    summarize_prompt = PromptTemplate.from_template(summarize_template)\n",
    "    summarize_chain = LLMChain(prompt=summarize_prompt, llm=llm)\n",
    "    return summarize_chain.run(content=chunk)\n",
    "\n",
    "def reduce_summaries(chunk_summaries):\n",
    "    combined_summary = \" \".join(chunk_summaries)\n",
    "\n",
    "    reduce_template = \"\"\"\n",
    "\n",
    "    {content}\n",
    "\n",
    "    FINAL SUMMARY:\"\"\"\n",
    "    reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "    reduce_chain = LLMChain(prompt=reduce_prompt, llm=llm)\n",
    "    return reduce_chain.run(content=combined_summary)\n",
    "\n",
    "def summarize_text_file(text_file_path, output_folder):\n",
    "    with open(text_file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    if not text:\n",
    "        print(f\"No text extracted from {text_file_path}\")\n",
    "        return None\n",
    "\n",
    "    text_chunks = chunk_text(text)\n",
    "    if not text_chunks:\n",
    "        print(f\"No chunks formed for {text_file_path}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Summarizing chunks for {text_file_path}...\")\n",
    "    chunk_summaries = [summarize_chunk(chunk) for chunk in text_chunks]\n",
    "\n",
    "    print(f\"Generating final summary for {text_file_path}...\")\n",
    "    final_summary = reduce_summaries(chunk_summaries)\n",
    "\n",
    "    output_file_path = os.path.join(output_folder, os.path.basename(text_file_path).replace(\".txt\", \"_summary.txt\"))\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(final_summary)\n",
    "\n",
    "    print(f\"Processed {text_file_path}\")\n",
    "\n",
    "    return final_summary\n",
    "\n",
    "def summarize_all_texts(text_folder_path):\n",
    "    text_files = [f for f in os.listdir(text_folder_path) if f.lower().endswith('.txt')]\n",
    "\n",
    "    all_summaries = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        future_to_text_file = {executor.submit(summarize_text_file, os.path.join(text_folder_path, text_file), SUMMARY_OUTPUT_FOLDER): text_file for text_file in text_files}\n",
    "\n",
    "        for future in as_completed(future_to_text_file):\n",
    "            text_file = future_to_text_file[future]\n",
    "            try:\n",
    "                summary = future.result()\n",
    "                if summary:\n",
    "                    all_summaries.append(summary)\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while processing {text_file}: {str(e)}\")\n",
    "\n",
    "    # Generate a final summary of all individual summaries\n",
    "    if all_summaries:\n",
    "        for i, summary in enumerate(all_summaries):\n",
    "            combined_output_path = os.path.join(SUMMARY_OUTPUT_FOLDER, f\"final_summary_{i+1}.txt\")\n",
    "            with open(combined_output_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(summary)\n",
    "            print(f\"Final Summary for {combined_output_path}:\")\n",
    "            print(summary)\n",
    "\n",
    "summarize_all_texts(TEXT_FOLDER_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "SUMMARY_FOLDER_PATH = \"summaries\"\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 200\n",
    "\n",
    "def read_summary_files(folder_path):\n",
    "    summaries = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\"_summary.txt\"):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                summaries.append(file.read())\n",
    "    return summaries\n",
    "\n",
    "def combine_summaries(summaries):\n",
    "    combined_text = \"\\n\\n\".join(summaries)\n",
    "    return combined_text\n",
    "\n",
    "def chunk_text(text, chunk_size, chunk_overlap):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "def summarize_chunks(chunks):\n",
    "    chunk_summaries = []\n",
    "    summary_template = \"\"\"Summarize the following text, highlighting the key points:\n",
    "\n",
    "    {text}\n",
    "\n",
    "    Summary:\"\"\"\n",
    "\n",
    "    for chunk in chunks:\n",
    "        prompt = PromptTemplate.from_template(summary_template)\n",
    "        chain = LLMChain(prompt=prompt, llm=llm, verbose=True)\n",
    "        summary = chain.run(text=chunk)\n",
    "        chunk_summaries.append(summary)\n",
    "\n",
    "    return chunk_summaries\n",
    "\n",
    "def combine_chunk_summaries(chunk_summaries):\n",
    "    combined_template = \"\"\"\n",
    "\n",
    "    {content}\n",
    "\n",
    "    FINAL COMPREHENSIVE SUMMARY:\"\"\"\n",
    "\n",
    "    combined_prompt = PromptTemplate.from_template(combined_template)\n",
    "    combined_chain = LLMChain(prompt=combined_prompt, llm=llm, verbose=True)\n",
    "    return combined_chain.run(content=\"\\n\\n\".join(chunk_summaries))\n",
    "\n",
    "# Read all summary files\n",
    "all_summaries = read_summary_files(SUMMARY_FOLDER_PATH)\n",
    "\n",
    "# Combine all summaries into one text\n",
    "combined_text = combine_summaries(all_summaries)\n",
    "\n",
    "# Chunk the combined text\n",
    "chunks = chunk_text(combined_text, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "\n",
    "# Summarize each chunk\n",
    "chunk_summaries = summarize_chunks(chunks)\n",
    "\n",
    "# Combine chunk summaries into a final summary\n",
    "final_summary = combine_chunk_summaries(chunk_summaries)\n",
    "\n",
    "# Print the final summary\n",
    "print(final_summary)\n",
    "\n",
    "# Save the final summary to a file\n",
    "output_path = os.path.join(SUMMARY_FOLDER_PATH, \"final_combined_summary.txt\")\n",
    "with open(output_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(final_summary)\n",
    "\n",
    "print(f\"Final combined summary saved to: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
